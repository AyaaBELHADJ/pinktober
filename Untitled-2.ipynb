{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "data= pd.read_csv(\"C:\\\\Users\\\\MASSIVE\\\\Downloads\\\\datathon\\\\train.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Marital_Status'] = data['Marital_Status'].map({'Married': 1, 'Single': 0})\n",
    "data['Radiation_Therapy'] = data['Radiation_Therapy'].map({'Yes': 1, 'No': 0})\n",
    "data['Chemotherapy'] = data['Chemotherapy'].map({'Yes': 1, 'No': 0})\n",
    "data['Hormone_Therapy'] = data['Hormone_Therapy'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age_Squared'] = data['Age'] ** 2\n",
    "\n",
    "data['Tumor_Sizes'] = data['Tumor_Size'] ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient_ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Year of Operation</th>\n",
       "      <th>Positive_Axillary_Nodes</th>\n",
       "      <th>Tumor_Size</th>\n",
       "      <th>Radiation_Therapy</th>\n",
       "      <th>Chemotherapy</th>\n",
       "      <th>Hormone_Therapy</th>\n",
       "      <th>Survival_Status</th>\n",
       "      <th>Age_Squared</th>\n",
       "      <th>Tumor_Sizes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1962</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5929</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1964</td>\n",
       "      <td>2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1296</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1960</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2209</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2916</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1968</td>\n",
       "      <td>5</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1225</td>\n",
       "      <td>16.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Patient_ID  Age  Marital_Status  Year of Operation  \\\n",
       "0           1   77               1               1962   \n",
       "1           2   36               1               1964   \n",
       "2           3   47               1               1960   \n",
       "3           4   54               1               1965   \n",
       "4           5   35               0               1968   \n",
       "\n",
       "   Positive_Axillary_Nodes  Tumor_Size  Radiation_Therapy  Chemotherapy  \\\n",
       "0                        5         3.0                  0             1   \n",
       "1                        2         1.9                  1             0   \n",
       "2                        5         2.0                  0             0   \n",
       "3                        0         1.4                  0             0   \n",
       "4                        5         4.1                  1             1   \n",
       "\n",
       "   Hormone_Therapy  Survival_Status  Age_Squared  Tumor_Sizes  \n",
       "0                0                1         5929         9.00  \n",
       "1                0                1         1296         3.61  \n",
       "2                0                0         2209         4.00  \n",
       "3                0                0         2916         1.96  \n",
       "4                1                1         1225        16.81  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.33%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.52      0.54       147\n",
      "           1       0.57      0.60      0.58       153\n",
      "\n",
      "    accuracy                           0.56       300\n",
      "   macro avg       0.56      0.56      0.56       300\n",
      "weighted avg       0.56      0.56      0.56       300\n",
      "\n",
      "Confusion Matrix:\n",
      " [[77 70]\n",
      " [61 92]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Splitting data into features and target\n",
    "X = data.drop(columns=['Survival_Status'])  # Features\n",
    "y = data['Survival_Status']  # Target\n",
    "\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=1)  # You can adjust the number of neighbors\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Optional: Print classification report and confusion matrix\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Patient_ID  Age Marital_Status  Year of Operation  Positive_Axillary_Nodes  \\\n",
      "0        1501   62         Single               1966                        4   \n",
      "1        1502   33        Married               1960                        4   \n",
      "2        1503   52        Married               1963                        7   \n",
      "3        1504   56        Married               1968                        7   \n",
      "4        1505   70        Married               1968                        1   \n",
      "\n",
      "   Tumor_Size Radiation_Therapy Chemotherapy Hormone_Therapy  \n",
      "0         1.4               Yes           No              No  \n",
      "1         3.8               Yes           No              No  \n",
      "2         2.1               Yes           No              No  \n",
      "3         1.6               Yes           No              No  \n",
      "4         4.1               Yes           No             Yes  \n"
     ]
    }
   ],
   "source": [
    "new_data = pd.read_csv('C:\\\\Users\\\\MASSIVE\\\\Downloads\\\\datathon\\\\test.csv')\n",
    "\n",
    "# Check the new data to confirm everything is in order\n",
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['Marital_Status'] = new_data['Marital_Status'].map({'Married': 1, 'Single': 0})\n",
    "new_data['Radiation_Therapy'] = new_data['Radiation_Therapy'].map({'Yes': 1, 'No': 0})\n",
    "new_data['Chemotherapy'] = new_data['Chemotherapy'].map({'Yes': 1, 'No': 0})\n",
    "new_data['Hormone_Therapy'] = new_data['Hormone_Therapy'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "\n",
    "new_data['Age_Squared'] = new_data['Age'] ** 2 \n",
    "\n",
    "\n",
    "new_data['Tumor_Size_Squared'] = new_data['Tumor_Size'] ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.drop(columns=[\"Patient_ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into features and target\n",
    "X = data.drop(columns=['Survival_Status'])  # Features\n",
    "y = data['Survival_Status']  # Target\n",
    "\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Logistic Regression - Accuracy: 0.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.56      0.53       147\n",
      "           1       0.53      0.47      0.50       153\n",
      "\n",
      "    accuracy                           0.51       300\n",
      "   macro avg       0.51      0.51      0.51       300\n",
      "weighted avg       0.51      0.51      0.51       300\n",
      "\n",
      "Training Decision Trees...\n",
      "Decision Trees - Accuracy: 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.48      0.48       147\n",
      "           1       0.51      0.52      0.52       153\n",
      "\n",
      "    accuracy                           0.50       300\n",
      "   macro avg       0.50      0.50      0.50       300\n",
      "weighted avg       0.50      0.50      0.50       300\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest - Accuracy: 0.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.56      0.53       147\n",
      "           1       0.53      0.46      0.49       153\n",
      "\n",
      "    accuracy                           0.51       300\n",
      "   macro avg       0.51      0.51      0.51       300\n",
      "weighted avg       0.51      0.51      0.51       300\n",
      "\n",
      "Training Support Vector Machines (SVM)...\n",
      "Support Vector Machines (SVM) - Accuracy: 0.49\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.53      0.51       147\n",
      "           1       0.50      0.46      0.48       153\n",
      "\n",
      "    accuracy                           0.49       300\n",
      "   macro avg       0.49      0.49      0.49       300\n",
      "weighted avg       0.49      0.49      0.49       300\n",
      "\n",
      "Training K-Nearest Neighbors (KNN)...\n",
      "K-Nearest Neighbors (KNN) - Accuracy: 0.53\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.48      0.50       147\n",
      "           1       0.54      0.58      0.56       153\n",
      "\n",
      "    accuracy                           0.53       300\n",
      "   macro avg       0.53      0.53      0.53       300\n",
      "weighted avg       0.53      0.53      0.53       300\n",
      "\n",
      "Training Gradient Boosting Machines (GBM)...\n",
      "Gradient Boosting Machines (GBM) - Accuracy: 0.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.59      0.55       147\n",
      "           1       0.54      0.46      0.49       153\n",
      "\n",
      "    accuracy                           0.52       300\n",
      "   macro avg       0.53      0.52      0.52       300\n",
      "weighted avg       0.53      0.52      0.52       300\n",
      "\n",
      "Training Naive Bayes...\n",
      "Naive Bayes - Accuracy: 0.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.61      0.55       147\n",
      "           1       0.53      0.41      0.46       153\n",
      "\n",
      "    accuracy                           0.51       300\n",
      "   macro avg       0.51      0.51      0.51       300\n",
      "weighted avg       0.51      0.51      0.51       300\n",
      "\n",
      "Training Neural Networks...\n",
      "Neural Networks - Accuracy: 0.49\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.46      0.47       147\n",
      "           1       0.50      0.52      0.51       153\n",
      "\n",
      "    accuracy                           0.49       300\n",
      "   macro avg       0.49      0.49      0.49       300\n",
      "weighted avg       0.49      0.49      0.49       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MASSIVE\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define your models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Trees': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machines (SVM)': SVC(),\n",
    "    'K-Nearest Neighbors (KNN)': KNeighborsClassifier(),\n",
    "    'Gradient Boosting Machines (GBM)': GradientBoostingClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Neural Networks': MLPClassifier(max_iter=500),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} - Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Store results\n",
    "    results[name] = y_pred\n",
    "    \n",
    "    # Optional: Detailed classification report\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MASSIVE\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 10 features, but MLPClassifier is expecting 11 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m mlp_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)  \u001b[38;5;66;03m# Ensure that you've trained the model before this line\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Make predictions on the new data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m y_pred_new \u001b[38;5;241m=\u001b[39m mlp_model\u001b[38;5;241m.\u001b[39mpredict(X_new_normalized)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for the predictions\u001b[39;00m\n\u001b[0;32m     28\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Survival_Status\u001b[39m\u001b[38;5;124m'\u001b[39m: y_pred_new})\n",
      "File \u001b[1;32mc:\\Users\\MASSIVE\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1156\u001b[0m, in \u001b[0;36mMLPClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict using the multi-layer perceptron classifier.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[38;5;124;03m    The predicted classes.\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(X)\n",
      "File \u001b[1;32mc:\\Users\\MASSIVE\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1160\u001b[0m, in \u001b[0;36mMLPClassifier._predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Private predict method with optional input validation\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1160\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pass_fast(X, check_input\u001b[38;5;241m=\u001b[39mcheck_input)\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1163\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[1;32mc:\\Users\\MASSIVE\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:202\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._forward_pass_fast\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict using the trained model\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03mThis is the same as _forward_pass but does not record the activations\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    The decision function of the samples for each class in the model.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_input:\n\u001b[1;32m--> 202\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Initialize first layer\u001b[39;00m\n\u001b[0;32m    205\u001b[0m activation \u001b[38;5;241m=\u001b[39m X\n",
      "File \u001b[1;32mc:\\Users\\MASSIVE\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\MASSIVE\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 10 features, but MLPClassifier is expecting 11 features as input."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the new data using the same scaler fitted on training data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Assuming you used the same features for training\n",
    "X_new = new_data.drop(columns=['Survival_Status'], errors='ignore')  # Drop target if it exists\n",
    "\n",
    "# Fit the scaler only on the training data (in a real case, you would not refit the scaler)\n",
    "# Use the original scaler that was fit on the training data\n",
    "X_new_normalized = scaler.fit_transform(X_new)\n",
    "\n",
    "# If you have already trained the neural network model\n",
    "# Use the MLPClassifier from your previous code\n",
    "mlp_model = MLPClassifier(max_iter=500)\n",
    "mlp_model.fit(X_train, y_train)  # Ensure that you've trained the model before this line\n",
    "\n",
    "# Make predictions on the new data\n",
    "y_pred_new = mlp_model.predict(X_new_normalized)\n",
    "\n",
    "# Create a DataFrame for the predictions\n",
    "predictions_df = pd.DataFrame({'Predicted_Survival_Status': y_pred_new})\n",
    "\n",
    "# Save predictions to a CSV file (optional)\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "# Display predictions\n",
    "print(predictions_df)\n",
    "\n",
    "# Optional: If you have labels for the new data, evaluate accuracy\n",
    "# If you have the true labels, you can uncomment the following lines:\n",
    "# true_labels = new_data['Survival_Status']  # Assuming you have this in your new data\n",
    "# accuracy = accuracy_score(true_labels, y_pred_new)\n",
    "# print(f'Accuracy on new data: {accuracy:.2f}')\n",
    "# print(classification_report(true_labels, y_pred_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"C:\\\\Users\\\\MASSIVE\\\\Downloads\\\\datathon\\\\test.csv\")\n",
    "\n",
    "# Step 2: Ensure that new_data and test_data have the same number of rows\n",
    "# This is important to avoid issues when adding the new column\n",
    "if len(test_data) != len(predictions_df):\n",
    "    raise ValueError(\"The number of rows in test_data and new_data do not match.\")\n",
    "\n",
    "# Step 3: Add the predictions from new_data as a new column in test_data\n",
    "test_data['Predicted_Survival_Status'] = predictions_df['Predicted_Survival_Status']\n",
    "test_data.to_csv(\"C:\\\\Users\\\\MASSIVE\\\\Downloads\\\\datathon\\\\test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped all columns except patientID and predicted survival status.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Load the existing test CSV file\n",
    "test_data = pd.read_csv(\"C:\\\\Users\\\\MASSIVE\\\\Downloads\\\\datathon\\\\test.csv\")\n",
    "\n",
    "# Step 2: Select only the patientID and the predicted survival status columns\n",
    "# Ensure you replace 'Predicted_Survival_Status' with the actual name if it differs\n",
    "filtered_test_data = test_data[['Patient_ID', 'Predicted_Survival_Status']]\n",
    "\n",
    "# Step 3: Save the updated test DataFrame back to CSV\n",
    "filtered_test_data.to_csv(\"C:\\\\Users\\\\MASSIVE\\\\Downloads\\\\datathon\\\\test.csv\", index=False)\n",
    "\n",
    "print(\"Dropped all columns except patientID and predicted survival status.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Predicted_Survival_Status\n",
      "0                            1\n",
      "1                            0\n",
      "2                            1\n",
      "3                            0\n",
      "4                            1\n",
      "..                         ...\n",
      "495                          0\n",
      "496                          0\n",
      "497                          1\n",
      "498                          1\n",
      "499                          0\n",
      "\n",
      "[500 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the new dataset (replace 'new_data.csv' with your actual CSV file name)\n",
    "\n",
    "\n",
    "# Normalize the new data using the same scaler fitted on training data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Assuming you used the same features for training\n",
    "X_new = new_data.drop(columns=['Survival_Status'], errors='ignore')  # Drop target if it exists, avoid error if it doesn't\n",
    "\n",
    "# Fit the scaler only on the training data (in a real case, you would not refit the scaler)\n",
    "# Use the original scaler that was fit on the training data\n",
    "X_new_normalized = scaler.fit_transform(X_new)\n",
    "\n",
    "# Load your trained KNN model (if you saved it earlier)\n",
    "# If not, you can use the model directly\n",
    "# For example, if you saved the model as 'knn_model.pkl':\n",
    "# import joblib\n",
    "# knn = joblib.load('knn_model.pkl')\n",
    "\n",
    "# Make predictions on the new data\n",
    "y_pred_new = knn.predict(X_new_normalized)\n",
    "\n",
    "# Create a DataFrame for the predictions\n",
    "predictions_df = pd.DataFrame({'Predicted_Survival_Status': y_pred_new})\n",
    "\n",
    "# Save predictions to a CSV file (optional)\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "# Display predictions\n",
    "print(predictions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best accuracy from grid search:  0.5083333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best accuracy from grid search: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.52\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.54      0.52       147\n",
      "           1       0.53      0.49      0.51       153\n",
      "\n",
      "    accuracy                           0.52       300\n",
      "   macro avg       0.52      0.52      0.52       300\n",
      "weighted avg       0.52      0.52      0.52       300\n",
      "\n",
      "Confusion Matrix:\n",
      "[[80 67]\n",
      " [78 75]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=190, random_state=24)\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Random Forest Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Feature  Importance\n",
      "12          Age_Times_Tumor_SizeSquared    0.088626\n",
      "8                  Age_Times_Tumor_Size    0.086682\n",
      "7    Tumor_Size_to_Positive_Nodes_Ratio    0.083979\n",
      "13  Tumor_Size_to_Positive_Nodes_RatioS    0.083595\n",
      "14                   Age_to_Year_Ratios    0.079164\n",
      "6                     Age_to_Year_Ratio    0.078596\n",
      "2                     Year of Operation    0.071150\n",
      "9                 Years_Since_Operation    0.067202\n",
      "15                          Tumor_Sizes    0.060826\n",
      "4                            Tumor_Size    0.058835\n",
      "3               Positive_Axillary_Nodes    0.057886\n",
      "10               Positive_Nodes_Squared    0.057429\n",
      "11                          Age_Squared    0.047557\n",
      "0                                   Age    0.046006\n",
      "1                        Marital_Status    0.018472\n",
      "5                     Radiation_Therapy    0.013995\n"
     ]
    }
   ],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "feature_names = X.columns  # Get feature names from your DataFrame\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(importance_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
